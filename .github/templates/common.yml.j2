{%- set upload_artifact_s3_action = "seemethere/upload-artifact-s3@v3" -%}

{# squid_proxy is an private ELB that only available for GHA custom runners #}
{%- set squid_proxy    = "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -%}
{# squid_no_proxy is a list of common set of fixed domains or IPs that we don't need to proxy. See https://docs.aws.amazon.com/AmazonECS/latest/developerguide/http_proxy_config.html#windows-proxy #}
{%- set squid_no_proxy = "localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" -%}
{%- set timeout_minutes = 240 -%}

{%- macro concurrency(build_environment) -%}
concurrency:
  {#- Explanation of the ciflow check: ciflow-triggered workflows use the push
  event. If we just use `github.sha` as the concurrency key, each push will get
  its own concurrency group. We want the behavior that new pushes of the ciflow
  tag to invalidate old ones, so we use the ciflow tag ref name as the key
  instead of the sha. #}
  group: !{{ build_environment }}-${{ github.event.pull_request.number || (startsWith(github.ref_name, 'ciflow') && github.ref_name || github.sha) }}-${{ github.event_name == 'workflow_dispatch' }}
  cancel-in-progress: true
{%- endmacro -%}

{%- macro pull_docker(image) -%}
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "!{{ image }}"
{%- endmacro -%}

{%- macro display_ec2_information() -%}
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
{%- endmacro -%}

{%- macro parse_ref() -%}
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
{%- endmacro -%}

{%- macro upload_test_statistics(build_environment, when="always()") -%}
      - name: Display and upload test statistics (Click Me)
        if: !{{ when }}
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: !{{ build_environment }}-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
{%- endmacro -%}

{%- macro setup_ec2_linux() -%}
      !{{ display_ec2_information() }}
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          !{{ pull_docker("${ALPINE_IMAGE}") }}
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
{%- endmacro -%}

{%- macro teardown_ec2_linux() -%}
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
{%- endmacro -%}

{%- macro checkout_pytorch(submodules) -%}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: !{{ submodules }}
{%- endmacro -%}

{%- macro upload_downloaded_files(name, config=None, shard=None, num_shards=None, runner=None, artifact_name="", use_s3=True, when="always()") -%}
      - name: Zip JSONs for upload
        if: !{{ when }}
        env:
{%- if name == 'linux' or name == 'windows' or name == 'macos' %}
          FILE_SUFFIX: '${{ github.job }}-!{{ config }}-!{{ shard }}-!{{ num_shards }}-!{{ runner }}'
{%- else %}
          FILE_SUFFIX: '!{{ name }}-${{ github.job }}'
{%- endif %}
{%- if name == 'windows' %}
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
{%- else %}
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
{%- endif %}
{%- if use_s3 %}
      - uses: !{{ upload_artifact_s3_action }}
        name: Store Test Downloaded JSONs on S3
{%- else %}
      - uses: actions/upload-artifact@v2
        name: Store Test Downloaded JSONs on Github
{%- endif %}
        if: !{{ when }}
        with:
{%- if artifact_name != "" %}
          name: !{{ artifact_name }}
{%- endif %}
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
{%- endmacro -%}

{%- macro upload_test_reports(name, config=None, shard=None, num_shards=None, runner=None, artifact_name="", use_s3=True) -%}
      - name: Zip test reports for upload
        if: always()
        env:
{%- if name == 'linux' or name == 'windows' or name == 'macos' %}
          FILE_SUFFIX: '${{ github.job }}-!{{ config }}-!{{ shard }}-!{{ num_shards }}-!{{ runner }}'
{%- else %}
          FILE_SUFFIX: '!{{ name }}-${{ github.job }}'
{%- endif %}
{%- if name == 'windows' %}
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
{%- else %}
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
{%- endif %}
{%- if use_s3 %}
      - uses: !{{ upload_artifact_s3_action }}
        name: Store Test Reports on S3
{%- else %}
      - uses: actions/upload-artifact@v2
        name: Store Test Reports on Github
{%- endif %}
        if: always()
        with:
{%- if artifact_name != "" %}
          name: !{{ artifact_name }}
{%- endif %}
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
{%- endmacro -%}

{%- macro render_test_results() -%}
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
{%- endmacro -%}

{%- macro calculate_docker_image(always_rebuild) -%}
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
{%- if not always_rebuild %}
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
{%- endif %}
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
{%- endmacro -%}

{%- macro setup_miniconda(python_version) -%}
      - name: Setup miniconda
        uses: conda-incubator/setup-miniconda@v2
        with:
          auto-update-conda: true
          python-version: !{{ python_version }}
          activate-environment: build
{%- endmacro -%}

{%- macro set_xcode_version(xcode_version) -%}
{%- if xcode_version != '' %}
      # Set xcode xcode version to !{{ xcode_version }}
      DEVELOPER_DIR: /Applications/Xcode_!{{ xcode_version }}.app/Contents/Developer
{%- endif %}
{%- endmacro -%}

{%- macro win_wait_and_kill_ssh() -%}
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
{%- endmacro %}

{%- macro upload_android_binary_size(build_type, artifacts) -%}
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          # The artifact file is created inside docker container, which contains the result binaries.
          # Now unpackage it into the project folder. The subsequent script will scan project folder
          # to locate result binaries and report their sizes.
          # If artifact file is not provided it assumes that the project folder has been mounted in
          # the docker during build and already contains the result binaries, so this step can be skipped.
          export ARTIFACTS=!{{ artifacts }}
          if [ -n "${ARTIFACTS}" ]; then
            tar xf "${ARTIFACTS}" -C "${GITHUB_WORKSPACE}"
            cd "${GITHUB_WORKSPACE}"
          fi
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          ANDROID_BUILD_TYPE=!{{ build_type}}
          export ANDROID_BUILD_TYPE
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba "android" || exit 0
{%- endmacro -%}

{%- macro build_android(env_name, container_suffix) -%}
      - name: Build-!{{ container_suffix }}
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          #!/bin/bash -eo pipefail
          # Pull Docker image and run build
          time docker pull "${DOCKER_IMAGE}" >/dev/null
          echo "${DOCKER_IMAGE}"
          export container_name
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT=!{{ env_name }} \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="!{{ squid_proxy }}" -e https_proxy="!{{ squid_proxy }}" -e no_proxy="!{{ squid_no_proxy }}" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          git submodule sync && git submodule update -q --init --recursive --depth 1 --jobs 0
          docker cp "${GITHUB_WORKSPACE}/." "${container_name}:/var/lib/jenkins/workspace"
          # shellcheck disable=SC1105
          ((echo "sudo chown -R jenkins . && .jenkins/pytorch/build.sh && find ${BUILD_ROOT} -type f -name "*.a" -or -name "*.o" -delete") | docker exec -u jenkins -i "${container_name}" bash) 2>&1

          # Copy dist folder back
          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}-!{{ container_suffix }}
          docker cp "${container_name}:/var/lib/jenkins/workspace/dist" "${GITHUB_WORKSPACE}/." || echo "Dist folder not found"
          docker commit "${container_name}" "${COMMIT_DOCKER_IMAGE}"
          time docker push "${COMMIT_DOCKER_IMAGE}"
{%- endmacro -%}
